{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e15343",
   "metadata": {},
   "source": [
    "# Data treatment and Neural Network Training using IPMA data\n",
    "João Oliveira and Edgar Mendes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c42abd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------Imports\n",
    "\n",
    "#Data request libraries\n",
    "import requests\n",
    "import json\n",
    "\n",
    "#Mathematics libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "import ipyplot \n",
    "import numpy as np\n",
    "\n",
    "#Time variables libraries\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import datetime\n",
    "import pytz\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "#File management libraries\n",
    "import io\n",
    "from io import BytesIO\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "#Progress bar libraries\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "#Image augmentation libraries\n",
    "import imageio\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "#Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras import layers\n",
    "from keras import callbacks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Flatten, Dropout, MaxPooling2D\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "#-----------------------Constantes\n",
    "\n",
    "#Recorte para cada distrito\n",
    "boxVianaDoCastelo = (570, 428, 770, 628)\n",
    "boxLeiria = (574, 902, 774, 1102)\n",
    "boxAveiro = (603, 687, 803, 887)\n",
    "boxBeja =  (735, 1546, 935, 1746)\n",
    "boxBraga =  (645, 463, 845, 663)\n",
    "boxBraganca = (953, 401, 1153, 601)\n",
    "boxCasteloBranco = (817, 886, 1017, 1086)\n",
    "boxPortalegre = (829, 1012, 1029, 1212)\n",
    "boxPorto = (611, 562, 811, 762)\n",
    "boxSantarem = (597, 1026, 797, 1226)\n",
    "boxCoimbra = (645, 792, 845, 992)\n",
    "boxEvora = (740, 1183, 940, 1383)\n",
    "boxFaro = (736, 1546, 936, 1746)\n",
    "boxGuarda = (859, 712, 1059, 912)\n",
    "boxLisboa = (513, 1149, 713, 1349)\n",
    "boxSetubal = (559, 1193, 759, 1393)\n",
    "boxVilaReal = (770, 527, 970, 727)\n",
    "boxViseu = (740, 682, 940, 882)\n",
    "\n",
    "#Dados das estações\n",
    "idVianaDoCastelo, idLeiria, idAveiro, idBeja, idBraga, idBraganca, idCasteloBranco, idPortalegre, idPorto, idSantarem, idCoimbra, idEvora, idFaro, idGuarda, idLisboa, idSetubal, idVilaReal, idViseu = 1240610, 1210718, 1210702, 1200562, 6212124, 1200575, 1200570, 1200571, 1240903, 1210734, 1210707, 1200558, 1200554, 1210683, 7240919, 1210770, 1240566, 1240675\n",
    "ids = np.array([1240610, 1210718, 1210702, 1200562, 6212124, 1200575, 1200570, 1200571, 1240903, 1210734, 1210707, 1200558, 1200554, 1210683, 7240919, 1210770, 1240566, 1240675])\n",
    "station_box_dict = {idVianaDoCastelo: boxVianaDoCastelo, idLeiria: boxLeiria, idAveiro: boxAveiro, idBeja: boxBeja, idBraga: boxBraga, idBraganca: boxBraganca, idCasteloBranco: boxCasteloBranco, idPortalegre: boxPortalegre, idPorto: boxPorto, idSantarem: boxSantarem, idCoimbra: boxCoimbra, idEvora: boxEvora, idFaro: boxFaro, idGuarda: boxGuarda, idLisboa: boxLisboa, idSetubal: boxSetubal, idVilaReal: boxVilaReal, idViseu: boxViseu}\n",
    "\n",
    "#Valores do augmentation\n",
    "hflip, vflip = iaa.Fliplr(p=1.0), iaa.Flipud(p=1.0)\n",
    "\n",
    "#Valores da rede neuronal\n",
    "N_CHANNELS = 4 \n",
    "N_CLASSES = 100 \n",
    "IMAGE_SIZE = 200\n",
    "\n",
    "#-----------------------Funções\n",
    "\n",
    "#Data request functions\n",
    "\n",
    "def get_data(url):\n",
    "    response = requests.get(f\"{url}\")\n",
    "    if response.status_code == 200:\n",
    "        #print(\"Sucessfully fetched the data!\") \n",
    "        return response.json() #https://www.educative.io/answers/how-to-make-api-calls-in-python\n",
    "    else:\n",
    "        print(f\"Hello there, there's a {response.status_code} error with your request.\")\n",
    "        \n",
    "def normalize_precipitation_value(precipitation_value):\n",
    "    return int(round((precipitation_value/240)*100,0)) #o valor normalizado ta a ser arredondado pq as pastas sao de valores inteiros. https://www.ipma.pt/pt/oclima/extremos.clima/ Vou usar o valor máximo aqui como referencia\n",
    "\n",
    "def list_dataset_folders():\n",
    "    folders = []\n",
    "    folder_path = os.getcwd()\n",
    "    for name in os.listdir(folder_path):\n",
    "        if os.path.isdir(os.path.join(folder_path, name)):\n",
    "            if name.endswith(\"dataset\"):\n",
    "                folders.append(name)\n",
    "    return folders\n",
    "\n",
    "def remove_black_pixels(image):\n",
    "    # Convert the image to RGBA mode (if it's not already in RGBA mode)\n",
    "    image = image.convert(\"RGBA\")\n",
    "    # Get the pixel data as a list of tuples\n",
    "    pixels = list(image.getdata())\n",
    "    # Replace every black pixel with transparent\n",
    "    new_pixels = []\n",
    "    for pixel in pixels:\n",
    "        if pixel[0] == 0 and pixel[1] == 0 and pixel[2] == 0:\n",
    "            new_pixels.append((0, 0, 0, 0))\n",
    "        else:\n",
    "            new_pixels.append(pixel)\n",
    "\n",
    "    # Create a new image with the same size and mode as the original image\n",
    "    new_image = Image.new(image.mode, image.size)\n",
    "    # Update the new image with the new pixel data\n",
    "    new_image.putdata(new_pixels)\n",
    "    # Return the new image\n",
    "    return new_image\n",
    "\n",
    "#Manage dictionaries and arrays\n",
    "\n",
    "def create_zero_dict(input_dict):\n",
    "    zero_dict = {key: 0 for key in input_dict} #cria um dicionario onde as chaves são as mesmas do dicionario que recebeu mas o value é 0 em todas\n",
    "    return zero_dict\n",
    "\n",
    "def get_dict_keys(input_dict):\n",
    "    keys_array = list(input_dict.keys())\n",
    "    return keys_array\n",
    "\n",
    "def dict_to_array1D(input_dict):\n",
    "    # Create an empty list to hold the values\n",
    "    output_array = np.empty(0)\n",
    "    # Loop over each key-value pair in the dictionary\n",
    "    for date_dict in input_dict.values():\n",
    "        for value in date_dict.values():\n",
    "            # Append the value to the output array\n",
    "            output_array = np.append(output_array,value)\n",
    "    # Return the output array\n",
    "    return output_array\n",
    "\n",
    "def array2D_to_array1D(array_2D):\n",
    "    array1D = np.ravel(array_2D)\n",
    "    return array1D\n",
    "\n",
    "def array1D_to_array2D(array_1D):\n",
    "    array2D = np.empty(0)\n",
    "    array2D = np.reshape(array_1D, (-1, 1))\n",
    "    return array2D\n",
    "\n",
    "#Dataset labels array management\n",
    "\n",
    "def count_how_many_occurences_of_each_value(arr):\n",
    "    counts = {}\n",
    "    for num in arr:\n",
    "        if num in counts:\n",
    "            counts[num] += 1\n",
    "        else:\n",
    "            counts[num] = 1\n",
    "    return counts\n",
    "\n",
    "def get_objective_hour():\n",
    "    # get current time\n",
    "    now = datetime.datetime.now()\n",
    "    # extract current hour from the current time\n",
    "    current_hour = now.hour\n",
    "    objective_hour = current_hour - 2\n",
    "    objective_hour = str(current_hour) + ':00'\n",
    "    return objective_hour\n",
    "\n",
    "def extract_hour_data(json_obj,objective_hour):    \n",
    "    hour_data = []\n",
    "    for date in json_obj:\n",
    "        if objective_hour in json_obj[date]:\n",
    "            hour_data.append(json_obj[date][objective_hour])\n",
    "    return hour_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "865bf502",
   "metadata": {},
   "outputs": [],
   "source": [
    "DADOS_ULTIMAS_3_HORAS = \"https://api.ipma.pt/open-data/observation/meteorology/stations/obs-surface.geojson\" #por exemplo se forem 9PM tem os dados entre 5PM e 7PM de hora a hora para todas as estações\n",
    "local_tz = pytz.timezone('Europe/Lisbon') #Define o fuso horário local\n",
    "\n",
    "def get_images_and_data_from_ipma():\n",
    "    \n",
    "    data = get_data(DADOS_ULTIMAS_3_HORAS)\n",
    "    specific_datasets = list_dataset_folders()\n",
    "    # Faz a requisição à apiEstacoes e à apiMeteo para obter as informações meteorológicas \n",
    "    final_result = {}\n",
    "    for feature in data['features']:\n",
    "        f.value+=1\n",
    "        if feature['properties']['idEstacao']  in ids:\n",
    "            station_data = feature['properties']\n",
    "            id_estacao = station_data['idEstacao']\n",
    "            # Converte a hora da estação para o fuso horário local\n",
    "            date_time_utc = datetime.fromisoformat(station_data['time'])\n",
    "            date_time = datetime.fromisoformat(station_data['time']).replace(tzinfo=pytz.utc).astimezone(local_tz)\n",
    "            date_str, hour_str = date_time.strftime('%Y-%m-%d %H:%M').split()\n",
    "            precipitation = station_data['precAcumulada']\n",
    "\n",
    "            if date_str not in final_result:\n",
    "                final_result[date_str] = {hour_str: precipitation}\n",
    "            else:\n",
    "                final_result[date_str][hour_str] = normalize_precipitation_value(max(0, precipitation))\n",
    "\n",
    "            url_image = f\"https://www.ipma.pt/resources.www/transf/radar/por/pcr-{date_time_utc.strftime('%Y-%m-%d')}T{date_time_utc.strftime('%H%M')}.png\"\n",
    "            response = requests.get(url_image)\n",
    "            image_data = io.BytesIO(response.content)\n",
    "            image = Image.open(image_data)\n",
    "            #print(f\"{url_image} ({hour_str}h)\")\n",
    "                    \n",
    "            # remover os pixeis pretos \n",
    "            image = remove_black_pixels(image)\n",
    "\n",
    "            # Cor-te da imagem\n",
    "            region = image.crop(station_box_dict[id_estacao])\n",
    "            for specific_dataset in specific_datasets:\n",
    "\n",
    "                # verifica se a pasta id_estacao existe e cria se não existir\n",
    "                if not os.path.exists(f\"{specific_dataset}/images/{id_estacao}\"):\n",
    "                    os.makedirs(f\"{specific_dataset}/images/{id_estacao}\")\n",
    "\n",
    "                # verifica se a pasta date_str existe e cria se não existir\n",
    "                if not os.path.exists(f\"{specific_dataset}/images/{id_estacao}/{date_str}\"):\n",
    "                    os.makedirs(f\"{specific_dataset}/images/{id_estacao}/{date_str}\")\n",
    "\n",
    "                #---- 135º----------------------------------------------------------\n",
    "                #certo\n",
    "                if specific_dataset == '135_rotated_dataset':\n",
    "                    rotated_135_region = region.rotate(135)\n",
    "                    rotated_135_region.save(f\"{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "\n",
    "                #certo\n",
    "                elif specific_dataset == '135_rotated_horizontal_and_vertical_flip_dataset':            \n",
    "                    img_np = np.array(region)\n",
    "                    input_hf = hflip.augment_image(img_np)\n",
    "                    input_vf = vflip.augment_image(input_hf)\n",
    "                    rotated_135_horizontal_and_vertical_flip_region = Image.fromarray(input_vf)\n",
    "                    rotated_135_horizontal_and_vertical_flip_region = rotated_135_horizontal_and_vertical_flip_region.rotate(135)\n",
    "                    rotated_135_horizontal_and_vertical_flip_region.save(f\"{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "\n",
    "                #certo\n",
    "                elif specific_dataset == '135_rotated_horizontal_flip_dataset':\n",
    "                    img_np = np.array(region)            \n",
    "                    input_hf = hflip.augment_image(img_np)\n",
    "                    rotated_135_horizontal_flip_region = Image.fromarray(input_hf)\n",
    "                    rotated_135_horizontal_flip_region = rotated_135_horizontal_flip_region.rotate(135)\n",
    "                    rotated_135_horizontal_flip_region.save(f\"{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "\n",
    "                #certo\n",
    "                elif specific_dataset == '135_rotated_vertical_flip_dataset':\n",
    "                    img_np = np.array(region)            \n",
    "                    input_vf = vflip.augment_image(img_np)\n",
    "                    rotated_135_vertical_flip_region = Image.fromarray(input_vf)\n",
    "                    rotated_135_vertical_flip_region = rotated_135_vertical_flip_region.rotate(135)\n",
    "                    rotated_135_vertical_flip_region.save(f\"{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "                \n",
    "                #---- 45º----------------------------------------------------------\n",
    "\n",
    "                #certo\n",
    "                elif specific_dataset == '45_rotated_dataset':\n",
    "                    rotated_45_region = region.rotate(45)\n",
    "                    rotated_45_region.save(f\"{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "\n",
    "                #certo\n",
    "                elif specific_dataset == '45_rotated_horizontal_and_vertical_flip_dataset':                    \n",
    "                    img_np = np.array(region)            \n",
    "                    input_hf = hflip.augment_image(img_np)\n",
    "                    input_vf = vflip.augment_image(input_hf)\n",
    "                    rotated_45_horizontal_and_vertical_flip_region = Image.fromarray(input_vf)\n",
    "                    rotated_45_horizontal_and_vertical_flip_region = rotated_45_horizontal_and_vertical_flip_region.rotate(45)\n",
    "                    rotated_45_horizontal_and_vertical_flip_region.save(f\"{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "\n",
    "                #certo\n",
    "                elif specific_dataset == '45_rotated_horizontal_flip_dataset':\n",
    "                    img_np = np.array(region)            \n",
    "                    input_hf = hflip.augment_image(img_np)\n",
    "                    rotated_45_horizontal_flip_region = Image.fromarray(input_hf)\n",
    "                    rotated_45_horizontal_flip_region = rotated_45_horizontal_flip_region.rotate(45)\n",
    "                    rotated_45_horizontal_flip_region.save(f\"{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "\n",
    "                #certo\n",
    "                elif specific_dataset == '45_rotated_vertical_flip_dataset':\n",
    "                    img_np = np.array(region)            \n",
    "                    input_vf = vflip.augment_image(img_np)\n",
    "                    rotated_45_vertical_flip_region = Image.fromarray(input_vf)\n",
    "                    rotated_45_vertical_flip_region = rotated_45_vertical_flip_region.rotate(45)\n",
    "                    rotated_45_vertical_flip_region.save(f\"{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "\n",
    "                #---- Só flips---------------------------------------------------------\n",
    "\n",
    "                #certo\n",
    "                elif specific_dataset == 'dataset':\n",
    "                    region.save(f\"{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "                \n",
    "                #certo\n",
    "                elif specific_dataset == 'horizontal_and_vertical_flip_dataset':\n",
    "                    img_np = np.array(region)            \n",
    "                    input_hf = hflip.augment_image(img_np)\n",
    "                    input_vf = vflip.augment_image(input_hf)\n",
    "                    horizontal_and_vertical_flip_region = Image.fromarray(input_vf)\n",
    "                    horizontal_and_vertical_flip_region.save(f\"{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "\n",
    "                #certo\n",
    "                elif specific_dataset == 'horizontal_flip_dataset':\n",
    "                    img_np = np.array(region)            \n",
    "                    input_hf = hflip.augment_image(img_np)\n",
    "                    horizontal_flip_region = Image.fromarray(input_hf)\n",
    "                    horizontal_flip_region.save(f\"{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "\n",
    "                #certo\n",
    "                elif specific_dataset == 'vertical_flip_dataset':\n",
    "                    img_np = np.array(region)            \n",
    "                    input_vf = vflip.augment_image(img_np)\n",
    "                    vertical_flip_region = Image.fromarray(input_vf)\n",
    "                    vertical_flip_region.save(f\"{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "\n",
    "                # Verifica se o arquivo JSON para esta estação já existe, se não, cria o arquivo\n",
    "                filename = f\"{specific_dataset}/precipitation/{id_estacao}.json\"\n",
    "                if not os.path.isfile(filename):\n",
    "                    with open(filename, 'w') as file:\n",
    "                        json.dump({}, file)\n",
    "\n",
    "                # Carrega o conteúdo do arquivo JSON para a variável \"precipitation_data\"\n",
    "                with open(filename, 'r') as file:\n",
    "                    precipitation_data = json.load(file)\n",
    "\n",
    "                # Adiciona as informações meteorológicas ao arquivo JSON\n",
    "                for date in final_result:\n",
    "                    if date not in precipitation_data:\n",
    "                        precipitation_data[date] = final_result[date]\n",
    "                    else:\n",
    "                        precipitation_data[date].update(final_result[date])\n",
    "\n",
    "                # Escreve o conteúdo atualizado no arquivo JSON\n",
    "                with open(filename, 'w') as file:\n",
    "                    json.dump(precipitation_data, file, indent=4)\n",
    "    print(\"Dados atualizados com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45cee6b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539149ec69e740af97ebead14a291a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=495)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados atualizados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "progress_bar_length = 495\n",
    "f = IntProgress(min=0, max=progress_bar_length) # instantiate the bar\n",
    "display(f) # display the bar\n",
    "get_images_and_data_from_ipma()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5b83ce",
   "metadata": {},
   "source": [
    "# Data treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "984abdd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba2d7f0969f426eb17ec80a9cb0faaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 1512\n"
     ]
    }
   ],
   "source": [
    "specific_datasets = list_dataset_folders()\n",
    "\n",
    "flag = False\n",
    "data_array = np.empty(0)\n",
    "images_array = np.empty((0, IMAGE_SIZE, IMAGE_SIZE, 4))\n",
    "\n",
    "\n",
    "#objective_hour = get_objective_hour()\n",
    "objective_hour = '12:00'\n",
    "images_end_with ='T'+ objective_hour[:2]+'00.png'\n",
    "\n",
    "for dataset in specific_datasets:\n",
    "    for stationJson in ids:\n",
    "        currentDir = dataset+'/precipitation/'+str(stationJson)+'.json'\n",
    "        with open(currentDir) as f:\n",
    "            # Load the JSON data\n",
    "            data = json.load(f)\n",
    "        f.close()\n",
    "        current_data_array = np.empty(0)\n",
    "        current_data_array = extract_hour_data(data,objective_hour)\n",
    "        data_array = np.concatenate((data_array, current_data_array))\n",
    "        if not flag:\n",
    "            flag = True\n",
    "            progress_bar_length = len(specific_datasets)*len(ids)*len(data_array)\n",
    "            progress_bar = IntProgress(min=0, max=progress_bar_length) # instantiate the bar\n",
    "            display(progress_bar) # display the bar\n",
    "    for stationFolder in ids:\n",
    "        images_folder_path = dataset+'/images/'+str(stationFolder)+'/'\n",
    "        # Get a list of all the files in the current folder\n",
    "        file_list = os.listdir(images_folder_path)\n",
    "        # Filter the list to only include folders files\n",
    "        days_folders_list = [file for file in file_list if os.path.isdir(os.path.join(images_folder_path, file))]\n",
    "        for day_folder in days_folders_list:\n",
    "            current_path = images_folder_path + day_folder\n",
    "            #Get a list of all files in the folder \n",
    "            files_list = os.listdir(current_path)\n",
    "            # Filter the list to only include images files\n",
    "            image_list = [file for file in files_list if file.endswith(images_end_with)]\n",
    "            for i in range(len(image_list)):\n",
    "                image_path = os.path.join(current_path, image_list[i])\n",
    "                #print(image_path)\n",
    "                image = Image.open(image_path)\n",
    "                img_np = np.array(image)\n",
    "                images_array = np.append(images_array,[img_np], axis=0) #NAO TIRAR PARENTESIS RETOS!!!!!!!!!!!!!!\n",
    "                progress_bar.value += 1\n",
    "                \n",
    "data_array = array1D_to_array2D(data_array)\n",
    "print(f\"Number of images: {len(images_array)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2703348d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec086d1f94ba4328be33afa3ba7a88f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=432)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 432\n"
     ]
    }
   ],
   "source": [
    "new_data_array = np.empty(0)\n",
    "new_images_array = np.empty((0, IMAGE_SIZE, IMAGE_SIZE, 4))\n",
    "\n",
    "how_many_of_each_value = count_how_many_occurences_of_each_value(array2D_to_array1D(data_array))\n",
    "smallest_value = min(how_many_of_each_value.values())\n",
    "dict_counter = create_zero_dict(how_many_of_each_value)\n",
    "\n",
    "how_many_different_values = len(how_many_of_each_value) * smallest_value\n",
    "\n",
    "f = IntProgress(min=0, max=how_many_different_values) # instantiate the bar\n",
    "display(f) # display the bar\n",
    "\n",
    "for index in range(len(data_array)):\n",
    "    if len(images_array) == how_many_different_values:\n",
    "        break\n",
    "    if dict_counter[data_array[index][0]] < smallest_value:\n",
    "        dict_counter[data_array[index][0]]+=1\n",
    "        new_data_array = np.append(new_data_array,data_array[index])\n",
    "        new_images_array = np.append(new_images_array,[images_array[index]], axis=0)\n",
    "        f.value+=1\n",
    "new_data_array = array1D_to_array2D(new_data_array)\n",
    "print(f\"Number of images: {len(new_data_array)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b9adf5",
   "metadata": {},
   "source": [
    "\n",
    "#  Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19f7acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perm = np.random.permutation(len(new_data_array))\n",
    "# new_images_array = new_images_array[perm]\n",
    "# new_data_array = new_data_array[perm]\n",
    "\n",
    "train_images, val_images, train_values, val_values = train_test_split(new_images_array, new_data_array, test_size=0.15, random_state=10)\n",
    "\n",
    "train_images = tf.keras.utils.normalize(train_images, axis=1)\n",
    "val_images = tf.keras.utils.normalize(val_images, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "380ce578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "102967424/102967424 [==============================] - 32s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "n_neuronios = 16\n",
    "filter_size = 3 \n",
    "max_pool_size = (2,2) \n",
    "n_epochs = 200 \n",
    "n_strides = 1\n",
    "dropout_value = 0.25 \n",
    "\n",
    "model = ResNet50()\n",
    "\n",
    "# model = tf.keras.models.Sequential()\n",
    "# model.add(layers.Conv2D(n_neuronios, max_pool_size, activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, N_CHANNELS)))\n",
    "# model.add(tf.keras.layers.Conv2D(n_neuronios, filter_size, strides=n_strides, padding='same', activation='relu'))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.Dropout(dropout_value))\n",
    "# model.add(tf.keras.layers.MaxPooling2D(pool_size=max_pool_size))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.Conv2D(N_NEURONIOS*4, FILTER_SIZE, padding='same', activation='relu'))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.MaxPooling2D(pool_size=MAX_POOL_SIZE))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.Conv2D(N_NEURONIOS*4, FILTER_SIZE, padding='same', activation='relu'))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.MaxPooling2D(pool_size=MAX_POOL_SIZE))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.Flatten())\n",
    "# model.add(tf.keras.layers.Dense(n_neuronios*2, activation='relu'))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.Dropout(dropout_value))\n",
    "# model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n",
    "\n",
    "# #model.summary()\n",
    "# #plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
    "\n",
    "# model.compile(\n",
    "#     loss='sparse_categorical_crossentropy',\n",
    "#     optimizer='adam',\n",
    "#     metrics=['accuracy']\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d6000ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 313031923247350500\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6435ea70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\applications\\resnet.py:159: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 4 input channels.\n",
      "  input_shape = imagenet_utils.obtain_input_shape(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "input_shape = (200, 200, 4) # input shape with 200x200 pixels and 4 channels for RGBA\n",
    "\n",
    "# create the EfficientNetB7 model\n",
    "model = ResNet50(include_top=True, weights=None, input_shape=input_shape, classes=100)\n",
    "\n",
    "# define the input tensor with the specified shape\n",
    "input_tensor = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "# pass the input tensor through the EfficientNetB7 model\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "# create the final model\n",
    "model = tf.keras.models.Model(inputs=input_tensor, outputs=output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b62d8a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', 'sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0fbaaea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12/12 [==============================] - 70s 5s/step - loss: 2.0628 - val_loss: 2.6512\n",
      "Epoch 2/200\n",
      " 8/12 [===================>..........] - ETA: 18s - loss: 0.5867"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\joao\\AppData\\Local\\Temp\\ipykernel_18176\\2303919647.py\", line 1, in <module>\n",
      "    history = model.fit(train_images, train_values, epochs= n_epochs, validation_data=(val_images, val_values))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1650, in fit\n",
      "    tmp_logs = self.train_function(iterator)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\", line 880, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\", line 912, in _call\n",
      "    return self._no_variable_creation_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\", line 134, in __call__\n",
      "    return concrete_function._call_flat(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\", line 1745, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\", line 378, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 52, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1543, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1501, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 755, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\ntpath.py\", line 647, in realpath\n",
      "    path = _getfinalpathname(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18176\\2303919647.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2076\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2078\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2077\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2078\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2079\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2080\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_values, epochs= n_epochs, validation_data=(val_images, val_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d03f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_hist(history):\n",
    "    plt.plot(history.history[\"accuracy\"])\n",
    "    plt.plot(history.history[\"val_accuracy\"])\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_hist(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ac139e95b93d37830bc6db1af5c1645ffabe82275972d7e78844595fc6ac7747"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
