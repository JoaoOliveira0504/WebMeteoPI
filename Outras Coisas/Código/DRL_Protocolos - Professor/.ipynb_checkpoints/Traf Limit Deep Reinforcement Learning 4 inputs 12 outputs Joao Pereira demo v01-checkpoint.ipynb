{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Limit with Deep Reinforcement Learning for 4 inputs and 12 outputs João Ferreira demo v02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------Imports-------------\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import reshape, shape\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Dropout_value = 0.3\n",
    "JSP = \"Glorot\"\n",
    "Max_fit_rep = 20 #numero maximo de ciclos\n",
    "Max_value = 0\n",
    "Num_trains = 5\n",
    "Num_hidden_layers = 1\n",
    "Num_neurons = 336\n",
    "Optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-3)\n",
    "Load_weights = 1 #primeira vez que faz o codigo por 0 de resto por 1\n",
    "Show_model = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafLimitEnv(Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(12)\n",
    "        low = np.array([-10, -10, -10, -10], dtype=np.float32)\n",
    "        high = np.array([10, 10, 10, 10], dtype=np.float32)        \n",
    "        self.observation_space = Box(low, high, dtype=np.float32)\n",
    "        self.state = None\n",
    "        self.Traf_length = 100\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        # 0 -1 = -1 decr limite\n",
    "        # 1 -1 = 0 sem acao\n",
    "        # 2 -1 = 1  incr limite\n",
    "        if action>=0 and action <=2 :\n",
    "            self.state[0] += 0.25*(action - 1) #trafego ICMP benigno\n",
    "        elif action>=3 and action <=5 :         \n",
    "            self.state[1] += 0.25*(action - 4) #trafego outros benigno\n",
    "        elif action>=6 and action <=8 :\n",
    "            self.state[2] += 0.25*(action - 7) #trafego ICMP maligno\n",
    "        elif action>=9 and action <=11 :         \n",
    "            self.state[3] += 0.25*(action - 10) #trafego outros maligno       \n",
    "            \n",
    "        reward = -1  # set default reward to -1\n",
    "        if 27-30 <= self.state[0] <= 33-30:\n",
    "            reward = 1  #Calculate reward do limite de trafego ICMP benigno  \n",
    "        if 27-30 <= self.state[1] <= 33-30:\n",
    "            reward = 1  # Calculate reward do limite de trafego outros benigno  \n",
    "        if 17-20 <= self.state[2] <= 23-20:\n",
    "            reward = 1  # Calculate reward do limite de trafego ICMP maligno  \n",
    "        if 17-20 <= self.state[3] <= 23-20:\n",
    "            reward = 1  # Calculate reward do limite de trafego outros maligno\n",
    "                    \n",
    "        self.Traf_length -= 1 # Reduce observation length by 1 second               \n",
    "        done = True if self.Traf_length <= 0 else False # Check if Traf is done\n",
    "            \n",
    "        info = {} # Set placeholder for info        \n",
    "        return self.state, reward, done, info # Return step information\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 25 + self.np_random.uniform(-4, 4, size=(4,))-25\n",
    "        self.Traf_length = 100 \n",
    "        return self.state\n",
    "    \n",
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(Flatten(input_shape=(1,states)))   \n",
    "    model.add(Dense(Num_neurons, activation='relu'))\n",
    "    for r in range(0, Num_hidden_layers):\n",
    "        model.add(Dense(Num_neurons, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "    nb_actions=actions, nb_steps_warmup=50, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Repetição Score nº  0\n",
      "Iteração:  1  - Pontuação para 10 episódios =  100.0\n",
      "                                                                   Reward Fit Glorot= 100.0\n",
      "max_w= 100.0\n",
      "{'episode_reward': [100.0], 'nb_episode_steps': [100], 'nb_steps': [100]}\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 100.000, steps: 100\n",
      "Episode 2: reward: 100.000, steps: 100\n",
      "Episode 3: reward: 100.000, steps: 100\n",
      "Episode 4: reward: 100.000, steps: 100\n",
      "Episode 5: reward: 100.000, steps: 100\n",
      "Episode 6: reward: 100.000, steps: 100\n",
      "Episode 7: reward: 100.000, steps: 100\n",
      "Episode 8: reward: 100.000, steps: 100\n",
      "Episode 9: reward: 100.000, steps: 100\n",
      "Episode 10: reward: 100.000, steps: 100\n",
      "                                                                   Reward Scores Glorot= 100.0\n",
      "________________________________________________________________________________\n",
      "Repetição Score nº  1\n",
      "Iteração:  1  - Pontuação para 10 episódios =  100.0\n",
      "                                                                   Reward Fit Glorot= 100.0\n",
      "max_w= 100.0\n",
      "{'episode_reward': [100.0], 'nb_episode_steps': [100], 'nb_steps': [100]}\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 100.000, steps: 100\n",
      "Episode 2: reward: 100.000, steps: 100\n",
      "Episode 3: reward: 100.000, steps: 100\n",
      "Episode 4: reward: 100.000, steps: 100\n",
      "Episode 5: reward: 100.000, steps: 100\n",
      "Episode 6: reward: 100.000, steps: 100\n",
      "Episode 7: reward: 100.000, steps: 100\n",
      "Episode 8: reward: 100.000, steps: 100\n",
      "Episode 9: reward: 100.000, steps: 100\n",
      "Episode 10: reward: 100.000, steps: 100\n",
      "                                                                   Reward Scores Glorot= 100.0\n",
      "________________________________________________________________________________\n",
      "Repetição Score nº  2\n",
      "Iteração:  1  - Pontuação para 10 episódios =  100.0\n",
      "                                                                   Reward Fit Glorot= 100.0\n",
      "max_w= 100.0\n",
      "{'episode_reward': [100.0], 'nb_episode_steps': [100], 'nb_steps': [100]}\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 100.000, steps: 100\n",
      "Episode 2: reward: 100.000, steps: 100\n",
      "Episode 3: reward: 100.000, steps: 100\n",
      "Episode 4: reward: 100.000, steps: 100\n",
      "Episode 5: reward: 100.000, steps: 100\n",
      "Episode 6: reward: 100.000, steps: 100\n",
      "Episode 7: reward: 100.000, steps: 100\n",
      "Episode 8: reward: 100.000, steps: 100\n",
      "Episode 9: reward: 100.000, steps: 100\n",
      "Episode 10: reward: 100.000, steps: 100\n",
      "                                                                   Reward Scores Glorot= 100.0\n",
      "________________________________________________________________________________\n",
      "Repetição Score nº  3\n",
      "Iteração:  1  - Pontuação para 10 episódios =  100.0\n",
      "                                                                   Reward Fit Glorot= 100.0\n",
      "max_w= 100.0\n",
      "{'episode_reward': [100.0], 'nb_episode_steps': [100], 'nb_steps': [100]}\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 100.000, steps: 100\n",
      "Episode 2: reward: 100.000, steps: 100\n",
      "Episode 3: reward: 100.000, steps: 100\n",
      "Episode 4: reward: 100.000, steps: 100\n",
      "Episode 5: reward: 100.000, steps: 100\n",
      "Episode 6: reward: 100.000, steps: 100\n",
      "Episode 7: reward: 100.000, steps: 100\n",
      "Episode 8: reward: 100.000, steps: 100\n",
      "Episode 9: reward: 100.000, steps: 100\n",
      "Episode 10: reward: 100.000, steps: 100\n",
      "                                                                   Reward Scores Glorot= 100.0\n",
      "________________________________________________________________________________\n",
      "Repetição Score nº  4\n",
      "Iteração:  1  - Pontuação para 10 episódios =  100.0\n",
      "                                                                   Reward Fit Glorot= 100.0\n",
      "max_w= 100.0\n",
      "{'episode_reward': [100.0], 'nb_episode_steps': [100], 'nb_steps': [100]}\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 100.000, steps: 100\n",
      "Episode 2: reward: 100.000, steps: 100\n",
      "Episode 3: reward: 100.000, steps: 100\n",
      "Episode 4: reward: 100.000, steps: 100\n",
      "Episode 5: reward: 100.000, steps: 100\n",
      "Episode 6: reward: 100.000, steps: 100\n",
      "Episode 7: reward: 100.000, steps: 100\n",
      "Episode 8: reward: 100.000, steps: 100\n",
      "Episode 9: reward: 100.000, steps: 100\n",
      "Episode 10: reward: 100.000, steps: 100\n",
      "                                                                   Reward Scores Glorot= 100.0\n"
     ]
    }
   ],
   "source": [
    "for n in range(0,Num_trains):\n",
    "    print(\"________________________________________________________________________________\")\n",
    "    print (\"Repetição Score nº \", n)\n",
    "    env = TrafLimitEnv()\n",
    "    states = env.observation_space.shape[0]\n",
    "    actions = env.action_space.n\n",
    "\n",
    "    model = build_model(states, actions)\n",
    "\n",
    "    if Show_model:\n",
    "        model.summary()\n",
    "\n",
    "    dqn = build_agent(model, actions)\n",
    "    dqn.compile(optimizer=Optimizer, metrics=['mae'])\n",
    "\n",
    "    cont=1\n",
    "\n",
    "    if Load_weights:\n",
    "        with open(\"Reward_\"+JSP+\"_max_dqn_weights.txt\", 'r') as f:\n",
    "            max_w = float(str(f.read()))\n",
    "    else:\n",
    "        with open(\"Reward_\"+JSP+\"_max_dqn_weights.txt\", 'w') as f:\n",
    "            f.write(\"0\")\n",
    "            max_w = 0 \n",
    "\n",
    "    with open(\"Rewards_\"+JSP+\".txt\", 'a') as f:\n",
    "        f.write('\\n')\n",
    "        \n",
    "    with open(\"Reward_\"+JSP+\"_max_dqn_weights.txt\", 'w') as f:\n",
    "        f.write(str(np.round(max_w,2)))\n",
    "\n",
    "    while True:\n",
    "        if Load_weights:\n",
    "            dqn.load_weights(JSP+\"_dqn_weights\")\n",
    "        hist = dqn.fit(env, nb_steps=100, visualize=False, verbose=0)\n",
    "        w = np.mean(hist.history['episode_reward'])\n",
    "        print(\"Iteração: \", cont, \" - Pontuação para 10 episódios = \", w)\n",
    "        with open(\"Rewards_\"+JSP+\".txt\", 'a') as f:\n",
    "            f.write('\\n'+str(np.round(w,2)))\n",
    "        if w >= max_w:\n",
    "            max_w = w\n",
    "            dqn.save_weights(JSP+\"_dqn_weights\", overwrite=True)\n",
    "            with open(\"Reward_\"+JSP+\"_max_dqn_weights.txt\", 'w') as f:\n",
    "                f.write(str(np.round(w,2)))     \n",
    "        if w >= 100 or cont >= Max_Fit_rep:\n",
    "            break\n",
    "        cont +=1\n",
    "\n",
    "    Reward_fit = np.mean(hist.history['episode_reward'])\n",
    "    print(\"                                                                   Reward Fit \"+JSP+\"=\", Reward_fit)  \n",
    "    with open(\"Reward_Fit_\"+JSP+\".txt\", 'a') as f:\n",
    "        f.write('\\n'+str(np.round(Reward_fit,2)))\n",
    "    print(\"max_w=\",max_w)  \n",
    "    print(hist.history)   \n",
    "    # After training is done, we save the final weights one more time.\n",
    "    del model\n",
    "    del dqn\n",
    "    del env\n",
    "    env = TrafLimitEnv()\n",
    "    states = env.observation_space.shape[0]\n",
    "    actions = env.action_space.n\n",
    "    model = build_model(states, actions)\n",
    "    dqn = build_agent(model, actions)\n",
    "    dqn.compile(optimizer=Optimizer, metrics=['mae'])\n",
    "    dqn.load_weights(JSP+\"_dqn_weights\")  #sem o .h5\n",
    "    scores = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "    scor = np.mean(scores.history['episode_reward'])\n",
    "    print(\"                                                                   Reward Scores \"+JSP+\"=\", scor)  \n",
    "    with open(\"Reward_Scores_\"+JSP+\".txt\", 'a') as f:\n",
    "        f.write('\\n'+str(np.round(scor,2)))\n",
    "    if scor >= Max_value:\n",
    "        Max_value = scor\n",
    "        dqn.save_weights(JSP+\"_dqn_weights_maxmax\", overwrite=True)\n",
    "        with open(\"Reward_\"+JSP+\"_score_dqn_weights_maxmax.txt\", 'w') as f:\n",
    "            f.write(str(np.round(scor,2))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
