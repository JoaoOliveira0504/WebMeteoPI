{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0e15343",
   "metadata": {},
   "source": [
    "# Data treatment and Neural Network Training using IPMA data\n",
    "João Oliveira and Edgar Mendes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c42abd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------Imports\n",
    "\n",
    "\n",
    "\n",
    "#Data request libraries\n",
    "import requests\n",
    "import json\n",
    "\n",
    "#Math libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "import ipyplot \n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#Time libraries\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pytz\n",
    "\n",
    "#File management libraries\n",
    "from io import BytesIO\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image, ImageDraw\n",
    "from collections import OrderedDict\n",
    "\n",
    "#Progress bar libraries\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "#Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks, models, layers, utils\n",
    "from tensorboard import notebook\n",
    "\n",
    "#-----------------------Constantes\n",
    "\n",
    "#Dados das estações\n",
    "\n",
    "boxVianaDoCastelo, boxLeiria, boxAveiro, boxBeja, boxBraga, boxBraganca, boxCasteloBranco, boxPortalegre, boxPorto, boxSantarem, boxCoimbra, boxEvora, boxFaro, boxGuarda, boxLisboa, boxSetubal, boxVilaReal, boxViseu = ((570, 428, 770, 628), (574, 902, 774, 1102), (603, 687, 803, 887), (735, 1546, 935, 1746), (645, 463, 845, 663), (953, 401, 1153, 601), (817, 886, 1017, 1086), (829, 1012, 1029, 1212), (611, 562, 811, 762), (597, 1026, 797, 1226), (645, 792, 845, 992), (740, 1183, 940, 1383), (736, 1546, 936, 1746), (859, 712, 1059, 912), (513, 1149, 713, 1349), (559, 1193, 759, 1393), (770, 527, 970, 727), (740, 682, 940, 882))\n",
    "idVianaDoCastelo, idLeiria, idAveiro, idBeja, idBraga, idBraganca, idCasteloBranco, idPortalegre, idPorto, idSantarem, idCoimbra, idEvora, idFaro, idGuarda, idLisboa, idSetubal, idVilaReal, idViseu = 1240610, 1210718, 1210702, 1200562, 6212124, 1200575, 1200570, 1200571, 1240903, 1210734, 1210707, 1200558, 1200554, 1210683, 7240919, 1210770, 1240566, 1240675\n",
    "ids = np.array([1240610, 1210718, 1210702, 1200562, 6212124, 1200575, 1200570, 1200571, 1240903, 1210734, 1210707, 1200558, 1200554, 1210683, 7240919, 1210770, 1240566, 1240675])\n",
    "station_box_dict = {idVianaDoCastelo: boxVianaDoCastelo, idLeiria: boxLeiria, idAveiro: boxAveiro, idBeja: boxBeja, idBraga: boxBraga, idBraganca: boxBraganca, idCasteloBranco: boxCasteloBranco, idPortalegre: boxPortalegre, idPorto: boxPorto, idSantarem: boxSantarem, idCoimbra: boxCoimbra, idEvora: boxEvora, idFaro: boxFaro, idGuarda: boxGuarda, idLisboa: boxLisboa, idSetubal: boxSetubal, idVilaReal: boxVilaReal, idViseu: boxViseu}\n",
    "\n",
    "#-----------------------Funções\n",
    "\n",
    "#Data request functions\n",
    "\n",
    "def get_data(url):\n",
    "    response = requests.get(f\"{url}\")\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Hello there, there's a {response.status_code} error with your request.\")\n",
    "        \n",
    "def normalize_precipitation_value(precipitation_value):\n",
    "    return int(round((precipitation_value/240)*100,0)) #o valor normalizado ta a ser arredondado pq as pastas sao de valores inteiros. https://www.ipma.pt/pt/oclima/extremos.clima/ Vou usar o valor máximo aqui como referencia\n",
    "\n",
    "def list_dataset_folders():\n",
    "    folders = []\n",
    "    folder_path = os.getcwd()\n",
    "    folder_path = os.path.join(folder_path, \"datasets\")\n",
    "    for name in os.listdir(folder_path):\n",
    "        if os.path.isdir(os.path.join(folder_path, name)):\n",
    "            if name.endswith(\"dataset\"):\n",
    "                folders.append(name)\n",
    "    return folders\n",
    "\n",
    "def remove_black_pixels(image):\n",
    "    # Convert the image to RGBA mode (if it's not already in RGBA mode)\n",
    "    image = image.convert(\"RGBA\")\n",
    "    # Get the pixel data as a list of tuples\n",
    "    pixels = list(image.getdata())\n",
    "    # Replace every black pixel with transparent\n",
    "    new_pixels = []\n",
    "    for pixel in pixels:\n",
    "        if pixel[0] == 0 and pixel[1] == 0 and pixel[2] == 0:\n",
    "            new_pixels.append((0, 0, 0, 0))\n",
    "        else:\n",
    "            new_pixels.append(pixel)\n",
    "    # Create a new image with the same size and mode as the original image\n",
    "    new_image = Image.new(image.mode, image.size)\n",
    "    # Update the new image with the new pixel data\n",
    "    new_image.putdata(new_pixels)\n",
    "    # Return the new image\n",
    "    return new_image\n",
    "\n",
    "#Manage dictionaries and arrays\n",
    "\n",
    "def dict_to_array1D(input_dict):\n",
    "    # Create an empty list to hold the values\n",
    "    output_array = np.empty(0)\n",
    "    # Loop over each key-value pair in the dictionary\n",
    "    for date_dict in input_dict.values():\n",
    "        for value in date_dict.values():\n",
    "            # Append the value to the output array\n",
    "            output_array = np.append(output_array,value)\n",
    "    # Return the output array\n",
    "    return output_array\n",
    "\n",
    "def array2D_to_array1D(array_2D):\n",
    "    return np.ravel(array_2D)\n",
    "\n",
    "def array1D_to_array2D(array_1D):\n",
    "    array2D = np.empty(0)\n",
    "    array2D = np.reshape(array_1D, (-1, 1))\n",
    "    return array2D\n",
    "\n",
    "def modify_dictionary_values(dictionary):\n",
    "    modified_dict = {}\n",
    "    for key, value in dictionary.items():\n",
    "        modified_dict[key] = min(value, 30)\n",
    "    return modified_dict\n",
    "\n",
    "def create_zero_dict(input_dict):\n",
    "    zero_dict = {key: 0 for key in input_dict} #cria um dicionario onde as chaves são as mesmas do dicionario que recebeu mas o value é 0 em todas\n",
    "    return zero_dict\n",
    "\n",
    "def count_how_many_occurrences_of_each_value(arr):\n",
    "    arr = array2D_to_array1D(arr)\n",
    "    counts = {}\n",
    "    for num in arr:\n",
    "        if num in counts:\n",
    "            counts[num] += 1\n",
    "        else:\n",
    "            counts[num] = 1\n",
    "    ordered_counts = OrderedDict(sorted(counts.items()))\n",
    "    return ordered_counts\n",
    "\n",
    "def get_one_hour_ago():\n",
    "    return (datetime.now() - timedelta(hours=1)).hour\n",
    "\n",
    "def extract_hour_data(json_obj,objective_hour):    \n",
    "    hour_data = []\n",
    "    for date in json_obj:\n",
    "        if objective_hour in json_obj[date]:\n",
    "            hour_data.append(json_obj[date][objective_hour])\n",
    "    return hour_data\n",
    "\n",
    "#Manage images\n",
    "\n",
    "def resize_image(image_path):\n",
    "    # Open the image file\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Resize the image to 100x100 pixels\n",
    "    resized_image = image.resize((100, 100))\n",
    "\n",
    "    # Return the resized image\n",
    "    return resized_image\n",
    "\n",
    "def shift_image_down(image_path, shift_value):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Create a new image with the updated height\n",
    "    new_image = Image.new(image.mode, (image.width, image.height))\n",
    "\n",
    "    # Copy the original image onto the new image, shifted down\n",
    "    new_image.paste(image, (0, shift_value))\n",
    "\n",
    "    # Get the first row of the original image\n",
    "    first_row = image.crop((0, 0, image.width, 1))\n",
    "\n",
    "    # Paste the first row into the new rows\n",
    "    for y in range(shift_value):\n",
    "        new_image.paste(first_row, (0, y, image.width, y + 1))\n",
    "\n",
    "    # Return the shifted and filled image\n",
    "    return new_image\n",
    "\n",
    "def shift_image_left(image_path, shift_value):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Create a new image with the updated width\n",
    "    new_image = Image.new(image.mode, (image.width, image.height))\n",
    "\n",
    "    # Copy the original image onto the new image, shifted to the left\n",
    "    new_image.paste(image, (-shift_value, 0))\n",
    "\n",
    "    # Get the last column of the original image\n",
    "    last_column = image.crop((image.width - 1, 0, image.width, image.height))\n",
    "\n",
    "    # Paste the last column into the new columns\n",
    "    for x in range(image.width - shift_value, image.width):\n",
    "        new_image.paste(last_column, (x, 0, x + 1, image.height))\n",
    "\n",
    "    image.close()\n",
    "    # Return the shifted and filled image\n",
    "    return new_image\n",
    "\n",
    "def shift_image_right(image_path, shift_value):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "        \n",
    "    # Create a new image with the updated width\n",
    "    new_image = Image.new(image.mode, (image.width, image.height))\n",
    "\n",
    "    # Copy the original image onto the new image, shifted to the right\n",
    "    new_image.paste(image, (shift_value, 0))\n",
    "\n",
    "    # Get the first column of the original image\n",
    "    first_column = image.crop((0, 0, 1, image.height))\n",
    "\n",
    "    # Paste the first column into the new columns\n",
    "    for x in range(shift_value):\n",
    "        new_image.paste(first_column, (x, 0, x + 1, image.height))\n",
    "\n",
    "    image.close()\n",
    "    # Return the shifted and filled image\n",
    "    return new_image\n",
    "\n",
    "def rotate_image(image_path, angle):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Rotate the image\n",
    "    rotated_image = image.rotate(angle, expand=True)\n",
    "\n",
    "    # Create a new image with RGBA mode and transparent background\n",
    "    new_image = Image.new(\"RGBA\", rotated_image.size, (0, 0, 0, 0))\n",
    "\n",
    "    # Paste the rotated image onto the new image\n",
    "    new_image.paste(rotated_image, (0, 0), rotated_image)\n",
    "\n",
    "    #Resize image to 200x200\n",
    "    new_image = new_image.resize((200,200))\n",
    "    \n",
    "    image.close()\n",
    "    # Return the rotated and filled image\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "865bf502",
   "metadata": {},
   "outputs": [],
   "source": [
    "DADOS_ULTIMAS_3_HORAS = \"https://api.ipma.pt/open-data/observation/meteorology/stations/obs-surface.geojson\"\n",
    "local_tz = pytz.timezone('Europe/Lisbon') #Define the current timezone\n",
    "\n",
    "def get_images_and_data_from_ipma():\n",
    "    \n",
    "    data = get_data(DADOS_ULTIMAS_3_HORAS)\n",
    "    specific_datasets = list_dataset_folders()\n",
    "    # Make the API call\n",
    "    final_result = {}\n",
    "    for feature in data['features']:\n",
    "        f.value+=1\n",
    "        if feature['properties']['idEstacao']  in ids:\n",
    "            station_data = feature['properties']\n",
    "            id_estacao = station_data['idEstacao']\n",
    "            # Convert the string date/time to a Python datetime object (in UTC+1)\n",
    "            date_time_utc = datetime.fromisoformat(station_data['time'])\n",
    "            date_time = datetime.fromisoformat(station_data['time']).replace(tzinfo=pytz.utc).astimezone(local_tz)\n",
    "            date_str, hour_str = date_time.strftime('%Y-%m-%d %H:%M').split()\n",
    "            precipitation = station_data['precAcumulada']\n",
    "\n",
    "            if date_str not in final_result:\n",
    "                final_result[date_str] = {hour_str: precipitation}\n",
    "            else:\n",
    "                final_result[date_str][hour_str] = normalize_precipitation_value(max(0, precipitation))\n",
    "\n",
    "            url_image = f\"https://www.ipma.pt/resources.www/transf/radar/por/pcr-{date_time_utc.strftime('%Y-%m-%d')}T{date_time_utc.strftime('%H%M')}.png\"\n",
    "            response = requests.get(url_image)\n",
    "            image_data = BytesIO(response.content)\n",
    "            image = Image.open(image_data)\n",
    "                    \n",
    "            # Remove the black pixels from the image\n",
    "            image = remove_black_pixels(image)\n",
    "            \n",
    "            # Cut the image to the station\n",
    "            region = image.crop(station_box_dict[id_estacao])\n",
    "            image.close()\n",
    "            for specific_dataset in specific_datasets:\n",
    "                # Verify if the station folder exists and create it if it doesn't\n",
    "                if not os.path.exists(f\"datasets/{specific_dataset}/images/{id_estacao}\"):\n",
    "                    os.makedirs(f\"datasets/{specific_dataset}/images/{id_estacao}\")\n",
    "\n",
    "                # Verify if the date folder exists and create it if it doesn't\n",
    "                if not os.path.exists(f\"datasets/{specific_dataset}/images/{id_estacao}/{date_str}\"):\n",
    "                    os.makedirs(f\"datasets/{specific_dataset}/images/{id_estacao}/{date_str}\")\n",
    "\n",
    "                # Save the image in the correct folder\n",
    "                elif specific_dataset == 'dataset':\n",
    "                    region.save(f\"datasets/{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "                    original_image_path = f\"datasets/dataset/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\"\n",
    "\n",
    "                elif specific_dataset == 'left_20px_shifted_dataset':        \n",
    "                    shift_image_left(original_image_path, 20).save(f\"datasets/{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "                    left_shifted_image_path = f\"datasets/left_20px_shifted_dataset/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\"\n",
    "\n",
    "                elif specific_dataset == 'right_20px_shifted_dataset':        \n",
    "                    shift_image_right(original_image_path, 20).save(f\"datasets/{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "                    right_shifted_image_path = f\"datasets/right_20px_shifted_dataset/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\"\n",
    "\n",
    "                elif specific_dataset == 'down_20px_shifted_dataset':\n",
    "                    shift_image_down(original_image_path, 20).save(f\"datasets/{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "                    down_shifted_image_path = f\"datasets/down_20px_shifted_dataset/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\"\n",
    "\n",
    "                elif specific_dataset == 'rotated_1degree__left_20px_shifted_dataset':\n",
    "                    rotate_image(left_shifted_image_path, 1).save(f\"datasets/{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "\n",
    "                elif specific_dataset == 'rotated_1degree__right_20px_shifted_dataset':\n",
    "                    rotate_image(right_shifted_image_path, 1).save(f\"datasets/{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "                \n",
    "                elif specific_dataset == 'rotated_1degree__down_20px_shifted_dataset':\n",
    "                    rotate_image(down_shifted_image_path, 1).save(f\"datasets/{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "\n",
    "                elif specific_dataset == 'rotated_1degree_dataset':\n",
    "                    rotate_image(original_image_path, 1).save(f\"datasets/{specific_dataset}/images/{id_estacao}/{date_str}/{date_time.strftime('%Y-%m-%dT%H%M')}.png\")\n",
    "\n",
    "                # Verify if the JSON file for this station already exists, if not, create the file\n",
    "                filename = f\"datasets/{specific_dataset}/precipitation/{id_estacao}.json\"\n",
    "                if not os.path.isfile(filename):\n",
    "                    with open(filename, 'w') as file:\n",
    "                        json.dump({}, file)\n",
    "\n",
    "                # Load the JSON file content to the \"precipitation_data\" variable\n",
    "                with open(filename, 'r') as file:\n",
    "                    precipitation_data = json.load(file)\n",
    "\n",
    "                # Add the weather information to the JSON file\n",
    "                for date in final_result:\n",
    "                    if date not in precipitation_data:\n",
    "                        precipitation_data[date] = final_result[date]\n",
    "                    else:\n",
    "                        precipitation_data[date].update(final_result[date])\n",
    "\n",
    "                # Write the updated content to the JSON file\n",
    "                with open(filename, 'w') as file:\n",
    "                    json.dump(precipitation_data, file, indent=4)\n",
    "    print(\"Dados atualizados com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45cee6b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df377481d9744b758f30a62dd779f876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=495)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados atualizados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "progress_bar_length = 495\n",
    "f = IntProgress(min=0, max=progress_bar_length) # instantiate the bar\n",
    "display(f) # display the bar\n",
    "get_images_and_data_from_ipma()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f5b83ce",
   "metadata": {},
   "source": [
    "# Data treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b40cdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many of each value in train:\n",
      "OrderedDict([(0.0, 3), (1.0, 4), (2.0, 4), (3.0, 5), (5.0, 6)])\n",
      "How many of each value in validation:\n",
      "OrderedDict([(0.0, 5), (1.0, 4), (2.0, 4), (3.0, 3), (5.0, 2)])\n"
     ]
    }
   ],
   "source": [
    "limit_dictionary = {key: 8 for key in range(101)} #para usar mais imagens de chuva, aumentar o valor do limite para cada categoria\n",
    "time_difference = 1 #diferença entre a hora da imagem e a hora do valor da chuva\n",
    "\n",
    "image_size = 100\n",
    "counter_dictionary = {key: 0 for key in range(101)}\n",
    "specific_datasets = list_dataset_folders()\n",
    "data_array = np.empty(0)\n",
    "images_array = np.empty((0, image_size, image_size, 4))\n",
    "\n",
    "np.random.shuffle(specific_datasets)\n",
    "np.random.shuffle(ids)\n",
    "\n",
    "for dataset in specific_datasets:\n",
    "    for stationID in ids:\n",
    "        currentDir = 'datasets/'+dataset+'/precipitation/'+str(stationID)+'.json'\n",
    "        with open(currentDir) as f:\n",
    "            # Load the JSON data\n",
    "            data = json.load(f)\n",
    "        f.close()\n",
    "        for date, hours in data.items():\n",
    "            for hour in hours:\n",
    "                hora_da_imagem = datetime.strptime(hour, \"%H:%M\")\n",
    "                # Subtract one hour\n",
    "                hora_da_imagem = hora_da_imagem - timedelta(hours=time_difference)\n",
    "                # Convert back to string\n",
    "                hora_da_imagem = hora_da_imagem.strftime(\"%H:%M\")\n",
    "                variable = date + 'T' + hora_da_imagem.replace(':', '') + '.png'  \n",
    "                value = data[date][hour]\n",
    "\n",
    "                if not os.path.exists('datasets/'+dataset+'/images/'+str(stationID)+'/'+date+'/'+variable):\n",
    "                    continue  # Skip the current iteration if the image doesn't exist\n",
    "                if counter_dictionary[value] < limit_dictionary[value]:\n",
    "                    # if value > 2: #para só usar imagens com classe 0,1,2 e identar corretamente o código\n",
    "                    #     continue\n",
    "                    # else:  \n",
    "                    counter_dictionary[value] += 1\n",
    "                    data_array = np.append(data_array, value)\n",
    "                    image = resize_image('datasets/'+dataset+'/images/'+str(stationID)+'/'+date+'/'+variable)\n",
    "                    img_np = np.array(image)\n",
    "                    image.close()\n",
    "                    images_array = np.append(images_array,[img_np], axis=0) #NAO TIRAR PARENTESIS RETOS!!!!!!!!!!!!!!\n",
    "\n",
    "perm = np.random.permutation(len(data_array))\n",
    "images_array, data_array = images_array[perm], data_array[perm]\n",
    "\n",
    "train_images, val_images, train_values, val_values = train_test_split(images_array, data_array, test_size=0.45, random_state=2)\n",
    "\n",
    "normalized_train_images = tf.keras.utils.normalize(train_images, axis=1)\n",
    "normalized_val_images = tf.keras.utils.normalize(val_images, axis=1)\n",
    "\n",
    "print(\"How many of each value in train:\")\n",
    "print(count_how_many_occurrences_of_each_value(train_values))\n",
    "print(\"How many of each value in validation:\")\n",
    "print(count_how_many_occurrences_of_each_value(val_values))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39b9adf5",
   "metadata": {},
   "source": [
    "\n",
    "#  Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "380ce578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2858 - accuracy: 0.7273\n",
      "Epoch 1: val_accuracy improved from -inf to 0.88889, saving model to model_weights_by_hour\\hour_difference_1_weights.h5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.2858 - accuracy: 0.7273 - val_loss: 0.8015 - val_accuracy: 0.8889\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7915 - accuracy: 0.8636\n",
      "Epoch 2: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 878ms/step - loss: 0.7915 - accuracy: 0.8636 - val_loss: 0.8274 - val_accuracy: 0.8889\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8506 - accuracy: 0.8182\n",
      "Epoch 3: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 883ms/step - loss: 0.8506 - accuracy: 0.8182 - val_loss: 0.8383 - val_accuracy: 0.8889\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5610 - accuracy: 0.9091\n",
      "Epoch 4: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 852ms/step - loss: 0.5610 - accuracy: 0.9091 - val_loss: 0.9153 - val_accuracy: 0.8889\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6002 - accuracy: 0.8636\n",
      "Epoch 5: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 859ms/step - loss: 0.6002 - accuracy: 0.8636 - val_loss: 1.0020 - val_accuracy: 0.8333\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7751 - accuracy: 0.9091\n",
      "Epoch 6: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 859ms/step - loss: 0.7751 - accuracy: 0.9091 - val_loss: 1.3263 - val_accuracy: 0.5000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4297 - accuracy: 0.8182\n",
      "Epoch 7: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 842ms/step - loss: 0.4297 - accuracy: 0.8182 - val_loss: 1.6464 - val_accuracy: 0.5556\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5866 - accuracy: 0.8636\n",
      "Epoch 8: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 846ms/step - loss: 0.5866 - accuracy: 0.8636 - val_loss: 2.0468 - val_accuracy: 0.5556\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4310 - accuracy: 0.9545\n",
      "Epoch 9: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 850ms/step - loss: 0.4310 - accuracy: 0.9545 - val_loss: 2.4276 - val_accuracy: 0.5556\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5002 - accuracy: 0.8636\n",
      "Epoch 10: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 867ms/step - loss: 0.5002 - accuracy: 0.8636 - val_loss: 2.9768 - val_accuracy: 0.5556\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3451 - accuracy: 0.9545\n",
      "Epoch 11: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 867ms/step - loss: 0.3451 - accuracy: 0.9545 - val_loss: 3.4187 - val_accuracy: 0.5000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4588 - accuracy: 0.8182\n",
      "Epoch 12: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 860ms/step - loss: 0.4588 - accuracy: 0.8182 - val_loss: 3.7770 - val_accuracy: 0.5000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2035 - accuracy: 1.0000\n",
      "Epoch 13: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 855ms/step - loss: 0.2035 - accuracy: 1.0000 - val_loss: 3.9634 - val_accuracy: 0.4444\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2674 - accuracy: 0.9545\n",
      "Epoch 14: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 843ms/step - loss: 0.2674 - accuracy: 0.9545 - val_loss: 4.0383 - val_accuracy: 0.4444\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5835 - accuracy: 0.8182\n",
      "Epoch 15: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 856ms/step - loss: 0.5835 - accuracy: 0.8182 - val_loss: 4.0080 - val_accuracy: 0.4444\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5146 - accuracy: 0.7727\n",
      "Epoch 16: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 831ms/step - loss: 0.5146 - accuracy: 0.7727 - val_loss: 3.8789 - val_accuracy: 0.4444\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2002 - accuracy: 0.9091\n",
      "Epoch 17: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 870ms/step - loss: 0.2002 - accuracy: 0.9091 - val_loss: 3.7125 - val_accuracy: 0.4444\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2152 - accuracy: 0.9545\n",
      "Epoch 18: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 862ms/step - loss: 0.2152 - accuracy: 0.9545 - val_loss: 3.5149 - val_accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0926 - accuracy: 1.0000\n",
      "Epoch 19: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 847ms/step - loss: 0.0926 - accuracy: 1.0000 - val_loss: 3.3213 - val_accuracy: 0.5000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1127 - accuracy: 0.9545\n",
      "Epoch 20: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 843ms/step - loss: 0.1127 - accuracy: 0.9545 - val_loss: 3.1302 - val_accuracy: 0.5000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1497 - accuracy: 1.0000\n",
      "Epoch 21: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 875ms/step - loss: 0.1497 - accuracy: 1.0000 - val_loss: 2.9282 - val_accuracy: 0.5000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1496 - accuracy: 1.0000\n",
      "Epoch 22: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 853ms/step - loss: 0.1496 - accuracy: 1.0000 - val_loss: 2.7666 - val_accuracy: 0.5000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1443 - accuracy: 0.9545\n",
      "Epoch 23: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 850ms/step - loss: 0.1443 - accuracy: 0.9545 - val_loss: 2.6633 - val_accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1374 - accuracy: 1.0000\n",
      "Epoch 24: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 866ms/step - loss: 0.1374 - accuracy: 1.0000 - val_loss: 2.6118 - val_accuracy: 0.5000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1954 - accuracy: 1.0000\n",
      "Epoch 25: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 842ms/step - loss: 0.1954 - accuracy: 1.0000 - val_loss: 2.5841 - val_accuracy: 0.5000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1278 - accuracy: 0.9545\n",
      "Epoch 26: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 832ms/step - loss: 0.1278 - accuracy: 0.9545 - val_loss: 2.5883 - val_accuracy: 0.5000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1842 - accuracy: 0.9545\n",
      "Epoch 27: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 847ms/step - loss: 0.1842 - accuracy: 0.9545 - val_loss: 2.6653 - val_accuracy: 0.5000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3641 - accuracy: 0.9091\n",
      "Epoch 28: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 864ms/step - loss: 0.3641 - accuracy: 0.9091 - val_loss: 2.7802 - val_accuracy: 0.5000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 1.0000\n",
      "Epoch 29: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 864ms/step - loss: 0.0832 - accuracy: 1.0000 - val_loss: 2.8909 - val_accuracy: 0.5556\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 1.0000\n",
      "Epoch 30: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 861ms/step - loss: 0.1276 - accuracy: 1.0000 - val_loss: 3.0241 - val_accuracy: 0.5556\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1377 - accuracy: 0.9545\n",
      "Epoch 31: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 850ms/step - loss: 0.1377 - accuracy: 0.9545 - val_loss: 3.1436 - val_accuracy: 0.5556\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2925 - accuracy: 0.9545\n",
      "Epoch 32: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 835ms/step - loss: 0.2925 - accuracy: 0.9545 - val_loss: 3.3164 - val_accuracy: 0.5556\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1469 - accuracy: 0.9091\n",
      "Epoch 33: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 849ms/step - loss: 0.1469 - accuracy: 0.9091 - val_loss: 3.5355 - val_accuracy: 0.5000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1790 - accuracy: 0.9545\n",
      "Epoch 34: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 844ms/step - loss: 0.1790 - accuracy: 0.9545 - val_loss: 3.7567 - val_accuracy: 0.5000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2073 - accuracy: 0.9091\n",
      "Epoch 35: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 858ms/step - loss: 0.2073 - accuracy: 0.9091 - val_loss: 3.9382 - val_accuracy: 0.4444\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1636 - accuracy: 0.9545\n",
      "Epoch 36: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 862ms/step - loss: 0.1636 - accuracy: 0.9545 - val_loss: 4.0903 - val_accuracy: 0.4444\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1493 - accuracy: 0.9545\n",
      "Epoch 37: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 850ms/step - loss: 0.1493 - accuracy: 0.9545 - val_loss: 4.2205 - val_accuracy: 0.4444\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 1.0000\n",
      "Epoch 38: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 861ms/step - loss: 0.0736 - accuracy: 1.0000 - val_loss: 4.3382 - val_accuracy: 0.4444\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 1.0000\n",
      "Epoch 39: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 856ms/step - loss: 0.0919 - accuracy: 1.0000 - val_loss: 4.4362 - val_accuracy: 0.4444\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1408 - accuracy: 0.9545\n",
      "Epoch 40: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 834ms/step - loss: 0.1408 - accuracy: 0.9545 - val_loss: 4.5163 - val_accuracy: 0.4444\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 1.0000\n",
      "Epoch 41: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 859ms/step - loss: 0.0609 - accuracy: 1.0000 - val_loss: 4.5656 - val_accuracy: 0.4444\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1436 - accuracy: 0.9091\n",
      "Epoch 42: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 854ms/step - loss: 0.1436 - accuracy: 0.9091 - val_loss: 4.6224 - val_accuracy: 0.4444\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1100 - accuracy: 1.0000\n",
      "Epoch 43: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 857ms/step - loss: 0.1100 - accuracy: 1.0000 - val_loss: 4.6601 - val_accuracy: 0.4444\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1554 - accuracy: 0.9091\n",
      "Epoch 44: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 837ms/step - loss: 0.1554 - accuracy: 0.9091 - val_loss: 4.6833 - val_accuracy: 0.4444\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2333 - accuracy: 0.9091\n",
      "Epoch 45: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 860ms/step - loss: 0.2333 - accuracy: 0.9091 - val_loss: 4.6167 - val_accuracy: 0.4444\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 1.0000\n",
      "Epoch 46: val_accuracy did not improve from 0.88889\n",
      "1/1 [==============================] - 1s 850ms/step - loss: 0.0732 - accuracy: 1.0000 - val_loss: 4.5559 - val_accuracy: 0.4444\n",
      "Epoch 46: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Valores da rede neuronal\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "weights_dir = \"model_weights_by_hour/hour_difference_\" + str(time_difference) + \"_weights.h5\"\n",
    "\n",
    "n_channels = 4 \n",
    "n_classes = 100 \n",
    "n_neuronios = 16\n",
    "filter_size = 3 \n",
    "max_pool_size = (2,2) \n",
    "n_epochs = 100 \n",
    "n_strides = 1\n",
    "dropout_value = 0.6\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(n_neuronios, max_pool_size, activation='relu', input_shape=(image_size, image_size, n_channels)))\n",
    "model.add(layers.Conv2D(n_neuronios, filter_size, strides=n_strides, padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(dropout_value))\n",
    "model.add(layers.MaxPooling2D(pool_size=max_pool_size))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(n_neuronios*2, filter_size, padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(n_neuronios*4, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(dropout_value))\n",
    "model.add(layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=False, verbose=1, start_from_epoch=30) #Stop training when a monitored metric has stopped improving\n",
    "model_checkpoint = callbacks.ModelCheckpoint(filepath=weights_dir, monitor='val_accuracy', save_best_only=True, verbose=1, mode='max', save_weights_only=True) #Callback to save the Keras model or model weights at some frequency\n",
    "terminate_on_nan = callbacks.TerminateOnNaN() #Callback that terminates training when a NaN (Not-a-number) loss is encountered\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1) \n",
    "\n",
    "#load best weights\n",
    "try:\n",
    "    model.load_weights(weights_dir)\n",
    "    print(\"Weights loaded\")\n",
    "except:\n",
    "    print(\"Weights not loaded\")\n",
    "finally:\n",
    "    #train the model with the callbacks\n",
    "    history = model.fit(normalized_train_images, train_values, epochs= n_epochs, validation_data=(normalized_val_images, val_values), callbacks=[early_stopping, model_checkpoint, terminate_on_nan, tensorboard_callback], verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
